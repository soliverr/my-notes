---
title: AI System Classification
id: ds1sug1q4u3ujkke87npubh
desc: Wellknown classification of AI systems
created: 1744907867267
updated: 1744907867267
tags:
  - artificial-intelligence
  - machine-learning
  - ai-classification
mindmap-plugin: basic
---

# Artificial Intelligence

## References
- [Artificial Intelligence Tutorial](https://www.geeksforgeeks.org/artificial-intelligence/)
- [Types of AI Based on Capabilities](https://www.geeksforgeeks.org/types-of-ai-based-on-capabilities-an-in-depth-exploration/)
- [Types of AI Based on Functionalities](https://www.geeksforgeeks.org/types-of-ai-based-on-functionalities/)
- [Approaches to AI Learning](https://www.geeksforgeeks.org/approaches-to-ai-learning/)

## Based on Capability
- Narrow AI (Weak AI)
- General AI (AGI)
- Superintelligent AI (ASI)

## Based on  Functionality
- Reactive Machines
- Limited Memory
- Theory of Mind
- Self-Aware AI

## Based on Learning Approach
- Machine Learning ^544f75e9-b789-a18f
	- Deep Learning
		- By Deep Architecture
			- Neural Networks
				- by Architecture
					- Fully Connected (Dense)
					- Convolutional (CNN)
					- Recurrent (RNN) ^4447e8fb-32a9-8532
						- LSTM
						- GRU
					- Transformers
						- by Core Task
							- Natural Language Processing (NLP) Centric
								- Decoder-only Transformers
									- GPT Family
										- GPT-2
										- GPT-3
										- GPT-4
										- GPT-Neo/GPT-J
									- LLaMA Family
									- Others
										- OPT
										- Bloom
										- Mistral
								- Encoder-only Transformers
									- BERT Family
										- BERT
										- RoBERTa
										- ALBERT
										- DistilBERT
									- Masked Language Modeling Focused
									- Feature Extraction Focused
								- Encoder-Decoder
								(a.k.a. Sequence-to-Sequence)
									- T5 Family
										- mT5 (Multilingual T5)
										- T5
									- BART Family
										- mBART
										- BART
									- Translation Models
									- Summarization Models
									- MarianMT
								- Mixture of Experts (MoE)
								- Multilingual Models ^d73e1e1f-fa61-fcd6
									- mBERT
									- XLM-R
									- mT5
								- RAG (Retrieval-Augmented Generation)
							- Vision Centric (Vision Transformers - ViTs) ^768c361f-bd66-bd69
								- Image Classification
								- Object Detection
								- Semantic Segmentation
								- Video Understanding
							- Multimodal
								- Text & Image
									- CLIP
									- DALL-E
									- Flamingo
								- Text & Audio
								- Text, Image & Video
									- Gato
							- Graph Transformers
							- Reinforcement Learning Transformers
							- Scientific Applications (e.g., Protein Folding)
						- by Scale and Training Strategy
							- Small-scale Transformers
							- Large Language Models (LLMs) - Parameter Scale (>1 Billion)
							- Foundation Models (Trained on broad data, adaptable)
							- Pre-training Objectives
								- Masked Language Modeling (MLM)
								- Causal Language Modeling (CLM)
								- Next Sentence Prediction (NSP) (less common now)
								- Span Corruption
								- Prefix Language Modeling
							- Fine-tuning Strategies
								- Task-specific fine-tuning
								- Instruction tuning
								- Prompt tuning
								- Parameter-efficient fine-tuning (PEFT)
						- by Openness and Availability ^9c40ad0a-c6c6-73a1
							- Proprietary Models (e.g., OpenAI models, Google models)
							- Open-Source Models (e.g., Llama, Mistral, OPT)
							- Research Models (often publicly shared for academic purposes)
						- by Architectural Variations
							- Attention Mechanism Modifications
								- Long-Range Attention Mechanisms (e.g., Reformer, Longformer)
								- Multi-Head Attention Variants
								- Sparse Attention
								- Efficient Attention Mechanisms
							- Positional Encoding Variations
								- Absolute Positional Embeddings
								- Relative Positional Embeddings (e.g., T5, RoBERTa)
								- Rotary Positional Embeddings (RoPE)
							- Normalization Layer Placement
								- Pre-Normalization
								- Post-Normalization
								- Sandwich Normalization
							- Activation Function Variations
								- ReLU and its variants (GeLU, Swish)
								- Other non-linearities
							- Layer Normalization vs. Batch Normalization
							- Residual Connections and their variations
						- ?Long-Context Transformers?
							- Longformer, BigBird, Claude 2, GPT-4 Turbo – optimized for long documents.
						- ?Efficient Transformers?
							- Linformer, Performer, Reformer – reduce memory/computational cost.
						- ?Instruction-Tuned / Chat Models?
							- ChatGPT, Claude, Gemini, Mistral-Instruct – optimized for dialogue.
					- Autoencoders
					- GAN
					- Graph (GNN)
				- by Information Flow ^61fda1b6-f98d-4c28
					- Feedforward
					- Recurrent
				- by Training
					- Supervised Learning ^833d18bd-6aae-cc48
						- k-NN
						- Linear models
						- Native Bayess classification
						- Decision trees
							- Random forest
							- Gradient boosting
						- Neural networks
					- Unsupervised Learning
						- Ordination
							- PCA
							- NMF
							- t-SNE
						- Clustering
							- k-Means
						- Agglom. Clust
							- DBSCAN
					- Semi-Supervised Learning
					- Self-Supervised Learning
					- Reinforcement Learning ^9d3ef6d6-1f4d-2d15
						- Q-Learning
							- DQNs
							- SARSA
							- DDPG
					- Transfer Learning
				- by Depth ^0432b037-8213-d353
					- Shallow
					- Deep
				- by Application ^55989a82-a1f3-2ff1
					- Computer Vision
					- Natural Language Processing (NLP)
					- Speech Recognition
					- Time Series Analysis
					- Recommender Systems
					- Medical Diagnostics
			- Non-Neural Deep Models
				- Deep Gaussian processes
				- Deep Kernel machines
				- Hierachical mixture models
	- Classical Machine Learning ^16e74507-1cb4-aac3
		- Regression
			- Linear
			- Polynomial
		- Classification
			- Logistic Regression
			- SVM
		- Decision trees
			- Random forest
			- Gradient boosting
		- Bayesian Methods
		- Clustering
			- K-Means
			- DBSCAN
		- Dimensionality Reduction
			- PCA
			- t-SNE
	- Learns from labeled data
	- Hybrid Learning
	- Embeddings
		- Text
			- Word2Vec, GloVe, FastText, BERT, GPT
		- Image
			- последний скрытый слой ResNet — это эмбеддинг.
		- Audio
			- wav2vec2, Whisper
		- Graphs
			- Node2Vec, DeepWalk, GraphSAGE
		- Категориальные признаки (табличные данные)
			- Пример (Keras):
			void
			void
			Embedding(input_dim=num_unique_categories, output_dim=embedding_dim)
		- Мультимодальные эмбеддинги
			- Пример: модель CLIP (от OpenAI) получает эмбеддинги изображения и текста в одном пространстве, чтобы можно было сравнивать их напрямую.
	- Tokenization
- Other Approaches
	- Symbolic AI ^695be396-67d0-256b
		- Expert Systems
		- Logic Programming
		- Knowledge-Based Systems
	- Evolutionary Computation ^8da0c8f5-b1ee-3e03
		- Genetic Algorithms
		- Ant Colony Optimization
		- Swarm Intelligence
	- Fuzzy Logic
	- Probabilistic Reasoning
		- Bayesian Networks
		- Markov Models
	- Planning and Decision Making
	- Robotics
	- Cognitive Systems

## Data preparation
- Очистка
	- удаление стоп-слов, пунктуации, приведение к нижнему регистру, токенизация.
- Преобразование в числовой формат
	- Bag of Words (BoW)
	- TF-IDF
	- Токенизация под трансформеры (например, BertTokenizer)
- Padding и Truncation: приведение длины всех последовательностей к одной.